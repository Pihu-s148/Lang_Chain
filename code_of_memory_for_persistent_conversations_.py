# -*- coding: utf-8 -*-
"""Code_of_Memory_for_Persistent_Conversations_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4pTDmjKlifKjt5vlEgKRteUp0W7qupi
"""

!pip install httpx==0.23.0

"""
### Step 1: Install Required Libraries

In the first cell of your Colab notebook, install the necessary libraries. LangChain and OpenAI are the primary libraries needed."""

!pip install -U langchain-openai

# Install LangChain
!pip install langchain

"""- **LangChain**: A framework for building applications with language models.

### Step 2:  Import Necessary Modules

After installing the library, import the necessary modules for your code.
"""

from langchain.memory import ConversationBufferMemory

from langchain_openai import ChatOpenAI

"""
### Step 3: Main Code
Here's the complete code to create a conversation memory, save a context, and load it."""

# Initialize the language model and memory
llm = ChatOpenAI(temperature=0, openai_api_key="your_api_key")
memory = ConversationBufferMemory()

# Simulate a conversation where memory is used
memory.save_context({"input": "What is AI?"}, {"output": "AI stands for Artificial Intelligence."})
memory.save_context({"input": "Tell me more about AI."}, {"output": "AI involves creating systems that can perform tasks requiring human intelligence."})

# Retrieve the conversation history stored in memory
print("Conversation History:")
print(memory.load_memory_variables({}))

"""
### Explanation of the Code

1. **Importing Modules**:

   - `ConversationBufferMemory` is imported from `langchain.memory`. This class is used to store and manage conversation history.
   
2. **Initialize Memory**:

   - `memory = ConversationBufferMemory()`: This creates an instance of `ConversationBufferMemory`, which will be used to store conversation data.
   
3. **Save Context**:

   - `memory.save_context({"input": "I like tech products"}, {"response": "Got it, tech products!"})`: This method saves a pair of input and response to the memory. Here, the input is "I like tech products" and the response is "Got it, tech products!".
   
4. **Load Memory Variables**:

   - `print(memory.load_memory_variables({}))`: This method retrieves the stored conversation data. The empty dictionary `{}` is used as a placeholder for any additional parameters that might be needed.
"""

# Example of continuing the conversation using memory
from langchain.schema import HumanMessage, AIMessage

# Create a list of messages, including the current user input
messages = [HumanMessage(content="What did we talk about?")]

# Retrieve the conversation history from memory
conversation_history = memory.load_memory_variables({})['history']

# Construct messages from conversation history
history_messages = []
for turn in conversation_history.split("\n"):
    if "Human:" in turn:
        history_messages.append(HumanMessage(content=turn.split("Human:")[-1].strip()))
    elif "AI:" in turn:
        history_messages.append(AIMessage(content=turn.split("AI:")[-1].strip()))

# Combine history messages with the current user input
messages = history_messages + messages

# Pass the updated messages to the language model
response = llm(messages=messages)
print("\nLLM Response:")
print(response)

"""
### Additional Resources

- **LangChain Documentation**: [LangChain Docs](https://langchain.readthedocs.io/en/latest/)
"""